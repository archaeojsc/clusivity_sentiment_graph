{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('piper_env': conda)"
  },
  "interpreter": {
   "hash": "a3159ad433684cc073e2f2886f0e3adf2c6c09c85c8664d251d37b953eb3fb7e"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "# docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
    "\n",
    "offline_tweets = 'Infrastructure BillSearchTerm - Infrastructure BillSearchTerm.csv' # Small data set\n",
    "# offline_tweets = 'initial_quote_infrastructurebillsince202119 - initial_quote_infrastructurebillsince202119.csv' # Large data set\n",
    "\n",
    "offline_tweets_df = pd.read_csv(offline_tweets, index_col= 0)\n",
    "\n",
    "# Process UTF-8 encoding\n",
    "offline_tweets_df['text'] = offline_tweets_df['text'].apply(lambda x: ast.literal_eval(x).decode('utf-8'))\n",
    "\n",
    "docs = offline_tweets_df['text']\n",
    "\n",
    "display(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model = BERTopic(n_gram_range = (1,3), verbose=True, calculate_probabilities = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_freq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_heatmap(n_clusters=3, top_n_topics=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_hierarchy(top_n_topics=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_term_rank(log_scale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_barchart(top_n_topics=9, n_words=5, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from operator import itemgetter\n",
    "import copy\n",
    "import itertools\n",
    "\n",
    "# most_valuable_edge Functions for Girvin-Newman\n",
    "from networkx.algorithms.centrality import edge_betweenness_centrality\n",
    "\n",
    "# Clustering Algorithms\n",
    "from networkx.algorithms.community.asyn_fluid import asyn_fluidc\n",
    "from networkx.algorithms.community.centrality import girvan_newman\n",
    "from networkx.algorithms.community.kernighan_lin import kernighan_lin_bisection\n",
    "from networkx.algorithms.community.modularity_max import greedy_modularity_communities\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "from networkx.algorithms.community.quality import performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from data\n",
    "\n",
    "doc_id = np.array(df['paper_id'])\n",
    "doc_topic = np.array(df['topic'])\n",
    "\n",
    "# Extract array of topic probabilities, no longer needed for latest .csv layout\n",
    "\n",
    "# topic_prob = []\n",
    "# for p in df['topic_prob']:\n",
    "#     row = [float(r) for r in p.strip('\\n').replace(\n",
    "#         '[', '').replace(']', '').split()]\n",
    "#     topic_prob.append(row)\n",
    "\n",
    "# topic_prob = np.array(topic_prob)\n",
    "\n",
    "topic_prob = np.array(df.iloc[:, (df.columns.get_loc('topic') + 1):])\n",
    "\n",
    "# Array of Topic labels\n",
    "topic_id = ['Topic_' + str(i) for i in range(topic_prob.shape[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_topic_prob = pd.DataFrame(\n",
    "    data=topic_prob,\n",
    "    index=np.array(doc_id),\n",
    "    columns=topic_id)\n",
    "df_topic_prob.insert(0, 'topic_id', doc_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UndirectedDocumentGraph():\n",
    "    \"\"\"\n",
    "    This class will be used to form various graph representations of our document corpus.\n",
    "    The graph representation can be created all at once or incrementally.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        # Create an empty, undirected NetworkX graph\n",
    "        self.nx_graph = nx.Graph()\n",
    "\n",
    "    ####################\n",
    "    # Graph Formation\n",
    "    ####################\n",
    "\n",
    "    def merge_graph_from_edgelist(\n",
    "            self,\n",
    "            path: str,\n",
    "            delimiter: str = \",\",\n",
    "            comments: str = \"#\"):\n",
    "        \"\"\"\n",
    "        Takes in an edge list from a delimited file, then unions it with current representation.\n",
    "        The graphs must be disjoint.\n",
    "        \"\"\"\n",
    "        # Load the new portion of the graph from file\n",
    "        new_graph = nx.read_weighted_edgelist(\n",
    "            path, delimiter=delimiter, comments=comments)\n",
    "\n",
    "        # Merge it with the existing representation\n",
    "        self.nx_graph = nx.union(new_graph, self.nx_graph)\n",
    "\n",
    "    def merge_graph_from_sparse_scipy(self, sp_matrix):\n",
    "        \"\"\"\n",
    "        Takes in a SciPy sparse matrix, representing our pair-wise document similarity, creates a new graph from\n",
    "        it, then merges with any existing nodes.\n",
    "        \"\"\"\n",
    "        # Load the new portion of the graph\n",
    "        new_graph = nx.from_scipy_sparse_matrix(\n",
    "            sp_matrix, parallel_edges=False, edge_attribute=\"weight\")\n",
    "\n",
    "        # An adjacency matrix will contain entries relating documents to themselves.\n",
    "        # These should be removed from the graph\n",
    "        new_graph.remove_edges_from(nx.selfloop_edges(new_graph))\n",
    "\n",
    "        # Merge it with the existing representation\n",
    "        self.nx_graph = nx.union(new_graph, self.nx_graph)\n",
    "\n",
    "    def merge_graph_from_numpy_matrix(self, np_matrix):\n",
    "        \"\"\"\n",
    "        Takes in a numpy matrix, representing our pair-wise document similarity, creates a new graph from\n",
    "        it, then merges with any existing nodes.\n",
    "        \"\"\"\n",
    "        # Load the new portion of the graph\n",
    "        new_graph = nx.from_numpy_matrix(np_matrix, parallel_edges=False)\n",
    "\n",
    "        # An adjacency matrix will contain entries relating documents to themselves.\n",
    "        # These should be removed from the graph\n",
    "        new_graph.remove_edges_from(nx.selfloop_edges(new_graph))\n",
    "\n",
    "        # Merge it with the existing representation\n",
    "        self.nx_graph = nx.union(new_graph, self.nx_graph)\n",
    "\n",
    "    def merge_graph_from_pandas_df(\n",
    "            self,\n",
    "            pandas_df,\n",
    "            source: str,\n",
    "            target: str,\n",
    "            edge_attr: str):\n",
    "        \"\"\"\n",
    "        Takes in a Pandas DataFrame consisting of two columns indicating the edge connection and\n",
    "        a column indicating what to use as a weight. Merges with the existing representation.\n",
    "        Any filtering of edges that should not be included in the graph should be done prior to calling\n",
    "        this function.\n",
    "        \"\"\"\n",
    "        # Load the new portion of the graph\n",
    "        new_graph = nx.from_pandas_edgelist(\n",
    "            pandas_df, source, target, edge_attr)\n",
    "\n",
    "        # Merge it with the existing representation\n",
    "        self.nx_graph = nx.union(new_graph, self.nx_graph)\n",
    "\n",
    "    def add_edges_with_default_weight(\n",
    "            self, edge_list: list, default_weight: float):\n",
    "        \"\"\"\n",
    "        Use this function to add edges to the graph all using some specified weight.\n",
    "        edge_list is expected to be a list of tuples: [(0, 1)]  # single edge (0,1)\n",
    "        ex. adding edges for citations to a graph already consisting of similarity weights\n",
    "        \"\"\"\n",
    "        # Set the weight for all edges to the default\n",
    "        edge_list_with_weight = [\n",
    "            (u, v, {\"weight\": default_weight}) for u, v in edge_list]\n",
    "\n",
    "        # Load the new portion of the graph\n",
    "        new_graph = nx.from_edgelist(edge_list_with_weight)\n",
    "\n",
    "        # Merge it with the existing representation\n",
    "        self.nx_graph = nx.union(new_graph, self.nx_graph)\n",
    "\n",
    "    def remove_edges_below_weight_threshold(self, threshold: float):\n",
    "        \"\"\"\n",
    "        Removes any edges less than (inclusive) the specified threshold\n",
    "        \"\"\"\n",
    "        edges_to_remove = [(u, v) for u, v, weight in self.nx_graph.edges.data(\n",
    "            \"weight\") if weight <= threshold]\n",
    "\n",
    "        self.nx_graph.remove_edges_from(edges_to_remove)\n",
    "\n",
    "    def remove_zero_weight_edges(self):\n",
    "        \"\"\"\n",
    "        Use this function to remove any edges that have a weight of 0, such as for documents that have zero relation.\n",
    "        \"\"\"\n",
    "        self.remove_edges_below_weight_threshold(0)\n",
    "\n",
    "    ####################\n",
    "    # Graph Similarity\n",
    "    ####################\n",
    "\n",
    "    def get_modularity(self, graph, communities):\n",
    "        \"\"\"\n",
    "        Calculates the modularity of a given partition of a graph. This will be one number for the whole partitioning.\n",
    "\n",
    "        NetworkX Doc:\n",
    "        https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.quality.modularity.html#networkx.algorithms.community.quality.modularity\n",
    "        \"\"\"\n",
    "        return modularity(graph, communities, weight=\"weight\")\n",
    "\n",
    "    def get_performance(self, graph, partition):\n",
    "        \"\"\"\n",
    "        The performance of a partition is the ratio of the number of intra-community edges plus\n",
    "        inter-community non-edges with the total number of potential edges\n",
    "\n",
    "        NetworkX Doc:\n",
    "        https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.quality.performance.html#networkx.algorithms.community.quality.performance\n",
    "        \"\"\"\n",
    "        return performance(graph, partition)\n",
    "\n",
    "    def girvan_newman(\n",
    "            self,\n",
    "            k: int,\n",
    "            most_valuable_edge: str = \"edge_betweenness_centrality\"):\n",
    "        \"\"\"\n",
    "        k - represents the number of tuples of communities from the algorithm\n",
    "        most_valuable_edge - function used to get the edge removed at each iteration\n",
    "\n",
    "        NetworkX Doc for the Girvan-Newman Method:\n",
    "        https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.centrality.girvan_newman.html#networkx.algorithms.community.centrality.girvan_newman\n",
    "\n",
    "        Helpful video explanation:\n",
    "        https://youtu.be/LtQoPEKKRYM\n",
    "        \"\"\"\n",
    "        if most_valuable_edge == \"edge_betweenness_centrality_equal_weight\":\n",
    "            # Default option for Girvan-Newman, assumes all edges of weight 1\n",
    "            comp = girvan_newman(self.nx_graph, edge_betweenness_centrality)\n",
    "        elif most_valuable_edge == \"edge_betweenness_centrality_equal_with_weight\":\n",
    "            # Take edge weight into account\n",
    "            def get_edge(G):\n",
    "                centrality = edge_betweenness_centrality(G, weight=\"weight\")\n",
    "                return max(centrality, key=centrality.get)\n",
    "            print(get_edge(self.nx_graph))\n",
    "            comp = girvan_newman(self.nx_graph, get_edge)\n",
    "        elif most_valuable_edge == \"least_similar\":\n",
    "            # Simple option of removing edge with least weight\n",
    "            def get_edge(G):\n",
    "                # Get edge based on the weight value (index 2 of triple)\n",
    "                u, v, w = min(G.edges(data=\"weight\"), key=itemgetter(2))\n",
    "                return (u, v)\n",
    "            comp = girvan_newman(self.nx_graph, get_edge)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Invalid most_valuable_edge option for Girvan-Newman\")\n",
    "\n",
    "        # Create a list of dictionaries representing each row for the Pandas DF\n",
    "        node_dict_list = []\n",
    "\n",
    "        # Extract only the first specified number of communities and add them\n",
    "        # to the dictionary\n",
    "        num_communities = 2\n",
    "        for communities in itertools.islice(comp, k):\n",
    "            # Get the n number of communities\n",
    "            community_tuple = tuple(sorted(c) for c in communities)\n",
    "\n",
    "            # Calculate the modularity of the partitioning\n",
    "            mod = self.get_modularity(self.nx_graph, community_tuple)\n",
    "\n",
    "            # Calculate the performance of the partitioning\n",
    "            perf = self.get_performance(self.nx_graph, community_tuple)\n",
    "\n",
    "            # Loop through each of the communities\n",
    "            for cluster_id in range(len(community_tuple)):\n",
    "                # Get the list of nodes in the community\n",
    "                nodes = community_tuple[cluster_id]\n",
    "\n",
    "                # Loop through each of the nodes and form the Pandas DF row\n",
    "                # dictionary\n",
    "                for node in nodes:\n",
    "                    row_dict = {\n",
    "                        \"algorithm\": \"Girvan-Newman\",\n",
    "                        \"settings\": \"most_valuable_edge:\" + most_valuable_edge,\n",
    "                        \"num_clusters\": num_communities,\n",
    "                        \"cluster_id\": cluster_id,\n",
    "                        \"node_id\": node,\n",
    "                        \"modularity\": mod,\n",
    "                        \"performance\": perf\n",
    "                    }\n",
    "                    node_dict_list.append(row_dict)\n",
    "\n",
    "            # Increment the community count\n",
    "            num_communities += 1\n",
    "\n",
    "        # Create a Pandas DF from the results\n",
    "        clustering_df = pd.DataFrame(node_dict_list)\n",
    "\n",
    "        return clustering_df\n",
    "\n",
    "    def greedy_modularity(self, min_edge_weight: float = 0.0):\n",
    "        \"\"\"\n",
    "        Does not take weight into account in the Modularity algorithm\n",
    "        min_edge_weight - Used to remove edges below a certain threshold prior to running algorithm.\n",
    "\n",
    "        NetworkX Doc:\n",
    "        https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.modularity_max.greedy_modularity_communities.html#networkx.algorithms.community.modularity_max.greedy_modularity_communities\n",
    "\n",
    "        Modularity on Wikipedia:\n",
    "        https://en.wikipedia.org/wiki/Modularity_(networks)\n",
    "        \"\"\"\n",
    "        # Remove edges below a certain threshold\n",
    "        edges_to_remove = [(u, v) for u, v, weight in self.nx_graph.edges.data(\n",
    "            \"weight\") if weight < min_edge_weight]\n",
    "        filtered_graph = copy.deepcopy(self.nx_graph)\n",
    "        filtered_graph.remove_edges_from(edges_to_remove)\n",
    "\n",
    "        # Divide the graph into communities\n",
    "        communities = greedy_modularity_communities(filtered_graph)\n",
    "\n",
    "        # Determine how many clusters we have\n",
    "        num_clusters = len(communities)\n",
    "\n",
    "        # Calculate the modularity of the partitioning\n",
    "        mod = self.get_modularity(filtered_graph, communities)\n",
    "\n",
    "        # Calculate the performance of the partitioning\n",
    "        perf = self.get_performance(filtered_graph, communities)\n",
    "\n",
    "        # Convert into a Pandas DF\n",
    "        # Create a list of dictionaries representing each row for the Pandas DF\n",
    "        node_dict_list = []\n",
    "\n",
    "        # Add a row to the DF for each individual node\n",
    "        for comm, cluster_id in zip(communities, range(num_clusters)):\n",
    "            # Loop through each of the nodes and form the Pandas DF row\n",
    "            # dictionary\n",
    "            for node in comm:\n",
    "                row_dict = {\n",
    "                    \"algorithm\": \"Greedy Modularity\",\n",
    "                    \"settings\": \"min_edge_weight:\" + str(min_edge_weight),\n",
    "                    \"num_clusters\": num_clusters,\n",
    "                    \"cluster_id\": cluster_id,\n",
    "                    \"node_id\": node,\n",
    "                    \"modularity\": mod,\n",
    "                    \"performance\": perf\n",
    "                }\n",
    "                node_dict_list.append(row_dict)\n",
    "\n",
    "        # Create a Pandas DF from the results\n",
    "        clustering_df = pd.DataFrame(node_dict_list)\n",
    "\n",
    "        return clustering_df\n",
    "\n",
    "    def async_fluidc(self, k: int, max_iter: int = 100):\n",
    "        \"\"\"\n",
    "        Does not support weighted graphs\n",
    "        k - number of communities to be found\n",
    "        max_iter - max number of iterations if algorithm doesn't converge\n",
    "\n",
    "        NetworkX Doc:\n",
    "        https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.asyn_fluid.asyn_fluidc.html#networkx.algorithms.community.asyn_fluid.asyn_fluidc\n",
    "        \"\"\"\n",
    "        # Find the communities based on the parameters\n",
    "        communities = asyn_fluidc(self.nx_graph, k, max_iter, seed=42)\n",
    "\n",
    "        # First need to convert iterator to list to act on it multiple times\n",
    "        comm_list = list(communities)\n",
    "\n",
    "        # Calculate the modularity of the partitioning\n",
    "        mod = self.get_modularity(self.nx_graph, comm_list)\n",
    "\n",
    "        # Calculate the performance of the partitioning\n",
    "        perf = self.get_performance(self.nx_graph, comm_list)\n",
    "\n",
    "        # Convert into a Pandas DF\n",
    "        # Create a list of dictionaries representing each row for the Pandas DF\n",
    "        node_dict_list = []\n",
    "\n",
    "        # Add a row to the DF for each individual node\n",
    "        for comm, cluster_id in zip(comm_list, range(k)):\n",
    "            # Loop through each of the nodes and form the Pandas DF row\n",
    "            # dictionary\n",
    "            for node in comm:\n",
    "                row_dict = {\n",
    "                    \"algorithm\": \"Fluid Communities\",\n",
    "                    \"settings\": \"max_iter:\" + str(max_iter),\n",
    "                    \"num_clusters\": k,\n",
    "                    \"cluster_id\": cluster_id,\n",
    "                    \"node_id\": node,\n",
    "                    \"modularity\": mod,\n",
    "                    \"performance\": perf\n",
    "                }\n",
    "                node_dict_list.append(row_dict)\n",
    "\n",
    "        # Create a Pandas DF from the results\n",
    "        clustering_df = pd.DataFrame(node_dict_list)\n",
    "\n",
    "        return clustering_df\n",
    "\n",
    "    def kernighan_lin_bisection(self, num_cuts: int, max_iter: int = 10):\n",
    "        \"\"\"\n",
    "        Cuts the graph into two groups at each iteration. Continues to do this recursively throughout each subgraph.\n",
    "        Will stop splitting the graph at num_cuts or when a subgraph can no longer be split, whichever comes first\n",
    "\n",
    "        num_cuts - Specifies the number of cut levels you want to make\n",
    "\n",
    "        NetworkX Doc:\n",
    "        https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.kernighan_lin.kernighan_lin_bisection.html#networkx.algorithms.community.kernighan_lin.kernighan_lin_bisection\n",
    "        \"\"\"\n",
    "        # Start with the entire graph\n",
    "        old_graphs = [self.nx_graph]\n",
    "        new_graphs = []\n",
    "\n",
    "        # Convert into a Pandas DF\n",
    "        # Create a list of dictionaries representing each row for the Pandas DF\n",
    "        node_dict_list = []\n",
    "\n",
    "        # Perform the cuts for the specified number of levels\n",
    "        for level in range(0, num_cuts):\n",
    "            # Check to ensure all graphs can be split. If not, then stop\n",
    "            # bisection.\n",
    "            min_graph_size = min([g.number_of_nodes() for g in old_graphs])\n",
    "            if min_graph_size <= 1:\n",
    "                break\n",
    "\n",
    "            # Keep track of all partitions on each level. Needed for\n",
    "            # calculating modularity/performance\n",
    "            level_partitions = []\n",
    "\n",
    "            # Loop through each graph representing one of the cuts\n",
    "            for g in old_graphs:\n",
    "                # Split each graph into 2\n",
    "                partitions = kernighan_lin_bisection(\n",
    "                    g, None, max_iter=max_iter, weight=\"weight\")\n",
    "\n",
    "                # Track the partitions for the level\n",
    "                level_partitions.extend(partitions)\n",
    "\n",
    "                # Prepare the graphs for the next level of bisection\n",
    "                for part in partitions:\n",
    "                    # Get the bisected half of the graph and create a new graph of only those nodes/edges\n",
    "                    # Will be used in the next iteration\n",
    "                    half_graph = g.subgraph(part)\n",
    "                    new_graphs.append(half_graph)\n",
    "\n",
    "            # Calculate the modularity of the partitioning, using all graphs at\n",
    "            # the level of cuts\n",
    "            mod = self.get_modularity(self.nx_graph, level_partitions)\n",
    "\n",
    "            # Calculate the performance of the partitioning\n",
    "            perf = self.get_performance(self.nx_graph, level_partitions)\n",
    "\n",
    "            # Add a row to the DF for each individual node\n",
    "            num_clusters = len(level_partitions)\n",
    "            for comm, cluster_id in zip(level_partitions, range(num_clusters)):\n",
    "                # Loop through each of the nodes and form the Pandas DF row\n",
    "                # dictionary\n",
    "                for node in comm:\n",
    "                    row_dict = {\n",
    "                        \"algorithm\": \"Kernighan-Lin Bisection\",\n",
    "                        \"settings\": \"max_iter:\" + str(max_iter),\n",
    "                        \"num_clusters\": num_clusters,\n",
    "                        \"cluster_id\": cluster_id,\n",
    "                        \"node_id\": node,\n",
    "                        \"modularity\": mod,\n",
    "                        \"performance\": perf\n",
    "                    }\n",
    "                    node_dict_list.append(row_dict)\n",
    "\n",
    "            old_graphs = new_graphs\n",
    "            new_graphs = []\n",
    "\n",
    "        # Create a Pandas DF from the results\n",
    "        clustering_df = pd.DataFrame(node_dict_list)\n",
    "\n",
    "        return clustering_df\n",
    "\n",
    "    def undirected_pagerank(self, alpha: float = 0.85, max_iter: int = 100):\n",
    "        \"\"\"\n",
    "        NetworkX Doc:\n",
    "        https://networkx.org/documentation/stable//reference/algorithms/generated/networkx.algorithms.link_analysis.pagerank_alg.pagerank.html\n",
    "        \"\"\"\n",
    "        # Get the dictionary output from the pagerank algorithm\n",
    "        pagerank_output = nx.pagerank(\n",
    "            self.nx_graph,\n",
    "            weight=\"weight\",\n",
    "            alpha=alpha,\n",
    "            max_iter=max_iter)\n",
    "\n",
    "        # Change the output to a Pandas DataFrame\n",
    "        node_id_list = []\n",
    "        page_rank_val_list = []\n",
    "        for node_id, page_rank_val in pagerank_output.items():\n",
    "            node_id_list.append(node_id)\n",
    "            page_rank_val_list.append(page_rank_val)\n",
    "\n",
    "        page_rank_df = pd.DataFrame.from_dict(data={\n",
    "            \"node_id\": node_id_list,\n",
    "            \"page_rank_val\": page_rank_val_list\n",
    "        })\n",
    "\n",
    "        # Sort by page rank value, descending\n",
    "        page_rank_df.sort_values(\n",
    "            by=['page_rank_val'],\n",
    "            ascending=False,\n",
    "            inplace=True)\n",
    "\n",
    "        return page_rank_df\n",
    "\n",
    "    ####################\n",
    "    # Output\n",
    "    ####################\n",
    "\n",
    "    def write_edge_list(\n",
    "            self,\n",
    "            path: str,\n",
    "            delimiter: str = \",\",\n",
    "            comments: str = \"#\"):\n",
    "        \"\"\"\n",
    "        Writes the current contents of the UndirectedDocumentGraph object to a NetworkX edge list representation\n",
    "        \"\"\"\n",
    "        nx.write_weighted_edgelist(\n",
    "            self.nx_graph,\n",
    "            path=path,\n",
    "            comments=comments,\n",
    "            delimiter=delimiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Probability threshold function\n",
    "\n",
    "\n",
    "def probThreshold(data, threshold: float = 0.01):\n",
    "    return np.where(data < threshold, 0, data)\n",
    "\n",
    "# Similarity measure\n",
    "\n",
    "\n",
    "def simAbsCorr(data):\n",
    "    S = np.absolute(np.corrcoef(data))\n",
    "    return S\n",
    "\n",
    "\n",
    "def simSignedCorr(data):\n",
    "    S = (1 + np.corrcoef(data)) / 2\n",
    "    return S\n",
    "\n",
    "# Adjacency functions\n",
    "\n",
    "\n",
    "def powerAdj(SimMat, Beta: int = 6):\n",
    "    A = SimMat ** Beta\n",
    "    np.fill_diagonal(A, 0)\n",
    "    return A\n",
    "\n",
    "\n",
    "def signumAdj(SimMat, tau: float = 0.0):\n",
    "    A = np.where(SimMat < tau, 0, 1)\n",
    "    np.fill_diagonal(A, 0)\n",
    "    return A\n",
    "\n",
    "# Topological Overlap Matrix function\n",
    "\n",
    "\n",
    "def TOMadjacency(AdjMat, threshold_quantile: float = 0.8):\n",
    "    '''\n",
    "    TOMadjacency calculates an adjacency matrix by the network overlap of nodes\n",
    "    in a weighted, undirected graph.\n",
    "    '''\n",
    "    # Calculate common neighbors of each node\n",
    "    L = AdjMat.dot(AdjMat.T)\n",
    "\n",
    "    # Calculate connectivity of node\n",
    "    Krow = AdjMat.sum(axis=1)\n",
    "    Kcol = AdjMat.sum(axis=0)\n",
    "    Kmin = np.array([np.minimum(k_i, Kcol) for k_i in Krow])\n",
    "\n",
    "    # Topological overlap\n",
    "    TOM = (L + AdjMat) / (Kmin + 1 - AdjMat)\n",
    "\n",
    "    TOM_filtered = np.where(\n",
    "        TOM >= np.quantile(\n",
    "            TOM, threshold_quantile), TOM, 0)\n",
    "\n",
    "    np.fill_diagonal(TOM_filtered, 0)\n",
    "\n",
    "    TOMlower = np.tril(TOM_filtered)\n",
    "\n",
    "    TOMsparse = csr_matrix(TOMlower)\n",
    "\n",
    "    return TOMsparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh_val = 1 / topic_prob.shape[1]\n",
    "\n",
    "topic_prob_sigProbs = probThreshold(topic_prob, threshold=thresh_val)\n",
    "\n",
    "zeroTopic_doc = np.where(topic_prob_sigProbs.sum(axis=1) == 0)[0].tolist()\n",
    "\n",
    "doc_kept = np.delete(doc_id, zeroTopic_doc)\n",
    "\n",
    "topic_prob_filtered = np.delete(topic_prob_sigProbs, zeroTopic_doc, axis=0)\n",
    "\n",
    "doc_topic_kept = np.delete(doc_topic, zeroTopic_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_topic = simSignedCorr(topic_prob_filtered.T)\n",
    "\n",
    "A_topic = signumAdj(S_topic, tau=np.quantile(S_topic, 0.8))\n",
    "\n",
    "TOM_topic = TOMadjacency(A_topic, threshold_quantile=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_graph = UndirectedDocumentGraph()\n",
    "\n",
    "topic_graph.merge_graph_from_sparse_scipy(TOM_topic)\n",
    "\n",
    "# Assign graph node names to topic id's\n",
    "topic_label_mapping = dict(zip(topic_graph.nx_graph, topic_id))\n",
    "topic_graph.nx_graph = nx.relabel_nodes(\n",
    "    topic_graph.nx_graph, topic_label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_graph = topic_graph\n",
    "\n",
    "output_dfs = []\n",
    "diagnostic_dfs = []\n",
    "\n",
    "print(\"Girvan-Newman (edge_betweenness_centrality_equal_with_weight):\")\n",
    "girvan_newman_df_ebcw = my_graph.girvan_newman(\n",
    "    20, \"edge_betweenness_centrality_equal_with_weight\")\n",
    "output_dfs.append(girvan_newman_df_ebcw)\n",
    "girvan_newman_df_ebcw_diag = girvan_newman_df_ebcw[[\n",
    "    'num_clusters', 'modularity', 'performance']].drop_duplicates(ignore_index=True)\n",
    "diagnostic_dfs.append(girvan_newman_df_ebcw_diag)\n",
    "print(girvan_newman_df_ebcw_diag, \"\\n\")"
   ]
  }
 ]
}