{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of GradientTwitterAnalysisExamples.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yies5RHtCagx"
      },
      "source": [
        "# Gradient Twitter Analysis Examples\n",
        "\n",
        "**Author: Newton Campbell** - newtonh20@ieee.org\n",
        "\n",
        "In this notebook, we demonstrate how to load the offline Twitter data for the summer semester Gradient project into a Python notebook. We then show a couple of examples of cleaning, sentiment analysis and how to form a graph based on semantic similarity of Tweets. Enjoy!\n",
        "\n",
        "## First, some references\n",
        "Okay, for those of you who haven't gotten a chance to play with NLP before, we are providing you a list of resources that we hope will be helpful:\n",
        "\n",
        "#### Libraries and open source resources\n",
        "* spaCy ([website](https://spacy.io/), [blog](https://explosion.ai/blog)) \\[Python; emerging open-source library with [fantastic usage examples](https://spacy.io/usage/spacy-101), [API documentation](https://spacy.io/api), and [demo applications](https://spacy.io/universe)\\]\n",
        "* Natural Language Toolkit (NLTK) ([website](https://www.nltk.org/), [book](https://www.nltk.org/book/)) \\[Python; practical intro to programming for NLP, mainly used for teaching\\]\n",
        "* Stanza CoreNLP ([website](https://stanfordnlp.github.io/stanza/)) \\[Python; high-quality analysis toolkit\\]\n",
        "* AllenNLP ([website](https://allennlp.org/)) \\[Python; NLP research library built on PyTorch\\]\n",
        "* Tensorflow Tutorials ([website](https://www.tensorflow.org/hub/tutorials)) \\[Python; Not the first thing folks think about with respect to this kind of NLP. But there are some interesting classification capabilities that may come in handy\\]\n",
        "* R Text Mining Libraries ([website](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html)) \\[You read that right; R has tons of open-source libraries that you can use for Text Analysis, [even some that are ports of Python libraries](https://towardsdatascience.com/r-packages-for-text-analysis-ad8d86684adb)\\]\n",
        "\n",
        "#### Oh, and can't forget graph libraries\n",
        "Ah, let's not forget about graph libraries. You will likely need one for this project. I would give you a list. But the one I would give you is just a subset of the one that [you would find here.](https://wiki.python.org/moin/PythonGraphLibraries). NetworkX and igraph are two of my go-to libraries. They have community detection (clustering for graphs) algorithms and are fairly straightforward to use.\n",
        "\n",
        "\n",
        "You will also want to play with some of the basic examples that <a href=\"https://towardsdatascience.com/getting-started-with-natural-language-processing-nlp-2c482420cc05\">can be found here</a> and look into the \"DIY projects and data sets\" section <a href=\"https://towardsdatascience.com/how-to-get-started-in-nlp-6a62aa4eaeff\">at the bottom of this page to really get your feet wet.</a> It can seem a little daunting with just how much there is to know at first. But just try to get a couple of working examples and remember to continue exploring through the semester in tandem to your work."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbTu6IVI14PA"
      },
      "source": [
        "# Install and import libraries\n",
        "\n",
        "Let's start by importing some libraries that will help with an analysis of Twitter data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isDcOfC-CZkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5898cdeb-78b1-47f9-d956-13305ac4a2a5"
      },
      "source": [
        "# !pip install langdetect\n",
        "# !pip install pycountry\n",
        "# !pip install emoji\n",
        "# !python -m spacy download en_core_web_md\n",
        "\n",
        "# Import Libraries\n",
        "from textblob import TextBlob\n",
        "import sys\n",
        "import tweepy\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import nltk\n",
        "# This is needed for parsing certain Tweets (You may need to download others for other datasets)\n",
        "nltk.download('vader_lexicon')\n",
        "import spacy\n",
        "import en_core_web_md\n",
        "nlp = en_core_web_md.load()\n",
        "# nlp = spacy.load('en_core_web_sm')           # A more detailed model (with higher-dimension word vectors) - 13s to load, normally \n",
        "import networkx as nx                        # a really useful network analysis library\n",
        "import pycountry\n",
        "import emoji\n",
        "import re\n",
        "import string\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from PIL import Image\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from langdetect import detect\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HCuyifqDc-h"
      },
      "source": [
        "# Load an Offline Dataset\n",
        "\n",
        "Now, let's load one of the offline datasets for the project. As stated in the project launch document, you will not have to use the Twitter API for this project. We've downloaded offline versions for you.\n",
        "\n",
        "**To run this code, you will have to change the value of the offline_tweets variable to the smallest file from Challenge 2:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "c4A2bkr54WC7",
        "outputId": "cc60aabd-011f-427f-e783-fa03acb7a99e"
      },
      "source": [
        "# Change this\n",
        "offline_tweets = 'Infrastructure BillSearchTerm - Infrastructure BillSearchTerm.csv'\n",
        "\n",
        "offline_tweets_df = pd.read_csv(offline_tweets)\n",
        "num_tweets = len(offline_tweets_df.index)\n",
        "display(offline_tweets_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgS46JY-ByEZ"
      },
      "source": [
        "## A Little Data Cleaning\n",
        "\n",
        "To properly evaluate the Tweets' semantic meaning, you usually have to clean up the text a little. Its just easier for most third-party libraries. But you should also see if a third-party library that you're using has its own cleaning function.\n",
        "\n",
        "**Here, we clean text by using lambda function and clean RT, link, punctuation characters and finally convert to lowercase.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8PX4oLRHBxm_",
        "outputId": "27a82818-c86c-49cf-bc52-fecfb0585763"
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "import ast\n",
        "\n",
        "tok = WordPunctTokenizer()\n",
        "pat1 = r'@[A-Za-z0-9]+'\n",
        "pat2 = r'https?://[A-Za-z0-9./]+'\n",
        "combined_pat = r'|'.join((pat1, pat2))\n",
        "def tweet_cleaner(text):\n",
        "    soup = BeautifulSoup(text, 'lxml')\n",
        "    souped = soup.get_text()\n",
        "    stripped = re.sub(combined_pat, '', souped)\n",
        "    try:\n",
        "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
        "    except:\n",
        "        clean = stripped\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
        "    lower_case = letters_only.lower()\n",
        "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
        "    # I will tokenize and join together to remove unneccessary white spaces\n",
        "    words = tok.tokenize(lower_case)\n",
        "    return (\" \".join(words)).strip()\n",
        "\n",
        "offline_tweets_df['text'] = offline_tweets_df['text'].apply(lambda x: ast.literal_eval(x).decode('utf-8'))\n",
        "offline_tweets_df['text'] = offline_tweets_df['text'].map(lambda x: tweet_cleaner(x))\n",
        "offline_tweets_df[\"text\"] = offline_tweets_df.text.str.lower()\n",
        "offline_tweets_df['text']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQo1ySlh6Hdk"
      },
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "Sentiment Analysis can help us decipher the mood and emotions of general public and gather insightful information regarding the context. Sentiment Analysis is a process of analyzing data and classifying it based on the need of the research. These sentiments can be used for a better understanding of various events and impact caused by it. [L. Bing](https://www.cs.uic.edu/~liub/FBS/SentimentAnalysis-and-OpinionMining.pdf) highlights that in the research literature it is possible to see many different names, e.g. “sentiment analysis, opinion mining, opinion extraction, sentiment mining, subjectivity analysis, affect analysis, emotion analysis, review mining”, however all of them have similar purposes and belong to the subject of sentiment analysis or opinion mining. By analysing these sentiments, we may find what people like, what they want and what their major concerns are.\n",
        "\n",
        "**Now we have a set of Tweets, loaded into a data frame, that we can mine for various purposes. Next, we will use Textblob to calculate positive, negative, neutral, polarity and compound parameters from the text.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlggR5fj6GrO",
        "outputId": "3d9d3de4-6552-4efc-fc8c-069e31f7cd27"
      },
      "source": [
        "#Sentiment Analysis\n",
        "def percentage(part,whole):\n",
        " return 100 * float(part)/float(whole)\n",
        "\n",
        "positive = 0\n",
        "negative = 0\n",
        "neutral = 0\n",
        "polarity = 0\n",
        "tweet_list = []\n",
        "neutral_list = []\n",
        "negative_list = []\n",
        "positive_list = []\n",
        "\n",
        "for index, tweet in offline_tweets_df.iterrows():\n",
        " \n",
        " #print(tweet.text)\n",
        " tweet_list.append(tweet.text)\n",
        " analysis = TextBlob(tweet.text)\n",
        " score = SentimentIntensityAnalyzer().polarity_scores(tweet.text)\n",
        " neg = score['neg']\n",
        " neu = score['neu']\n",
        " pos = score['pos']\n",
        " comp = score['compound']\n",
        " polarity += analysis.sentiment.polarity\n",
        " \n",
        " if neg > pos:\n",
        "  negative_list.append(tweet.text)\n",
        "  negative += 1\n",
        " elif pos > neg:\n",
        "  positive_list.append(tweet.text)\n",
        "  positive += 1\n",
        " elif pos == neg:\n",
        "  neutral_list.append(tweet.text)\n",
        "  neutral += 1\n",
        "\n",
        "positive = percentage(positive, num_tweets)\n",
        "negative = percentage(negative, num_tweets)\n",
        "neutral = percentage(neutral, num_tweets)\n",
        "polarity = percentage(polarity, num_tweets)\n",
        "positive = format(positive, '.1f')\n",
        "negative = format(negative, '.1f')\n",
        "neutral = format(neutral, '.1f')\n",
        "\n",
        "#Number of Tweets (Total, Positive, Negative, Neutral)\n",
        "tweet_list = pd.DataFrame(tweet_list)\n",
        "neutral_list = pd.DataFrame(neutral_list)\n",
        "negative_list = pd.DataFrame(negative_list)\n",
        "positive_list = pd.DataFrame(positive_list)\n",
        "print(\"Total Tweets: \",len(tweet_list))\n",
        "print(\"positive number: \",len(positive_list))\n",
        "print(\"negative number: \", len(negative_list))\n",
        "print(\"neutral number: \",len(neutral_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ThZ791M4Vbm"
      },
      "source": [
        "**We can create a straightforward pie chart to profile the data in a more meaningful way:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "SEsiCf0O9jOY",
        "outputId": "5f67a6b7-23cb-4d6a-ca86-3775708f5873"
      },
      "source": [
        "#Creating PieCart\n",
        "labels = ['Positive ['+str(positive)+'%]' , 'Neutral ['+str(neutral)+'%]','Negative ['+str(negative)+'%]']\n",
        "sizes = [positive, neutral, negative]\n",
        "colors = ['yellowgreen', 'blue','red']\n",
        "patches, texts = plt.pie(sizes,colors=colors, startangle=90)\n",
        "plt.style.use('default')\n",
        "plt.legend(labels)\n",
        "plt.title(\"Sentiment Analysis Result for short list of Infrastructure Bill Tweets\")\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5x5XrIp798sL"
      },
      "source": [
        "**Using the Piper SME Political Typology described in the Project Launch document, let's take a look at this chart for Democrats(ID=-2) and Republicans(ID=2)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 839
        },
        "id": "EuWFYuZa-e31",
        "outputId": "60538be1-14e9-45b9-9307-b49ba0e0201a"
      },
      "source": [
        "# First, let's establish the Typology dictionary\n",
        "typology_dict = {'Fringe Left' : -3, 'Progressive' : -2, 'Democrat' : -1, 'Centrist' : 0, 'Libertarian' : 1, 'Republican' : 2, 'Trump-Republican' : 2.5, 'Fringe Right' : 3}\n",
        "\n",
        "for group in [typology_dict['Democrat'],typology_dict['Republican']]:\n",
        "  positive = 0\n",
        "  negative = 0\n",
        "  neutral = 0\n",
        "  polarity = 0\n",
        "  tweet_list = []\n",
        "  neutral_list = []\n",
        "  negative_list = []\n",
        "  positive_list = []\n",
        "  rows_in_group = len(offline_tweets_df[offline_tweets_df['tweet category'] == group].index)\n",
        "  for index, tweet in offline_tweets_df[offline_tweets_df['tweet category'] == group].iterrows():\n",
        "    #print(tweet.text)\n",
        "    tweet_list.append(tweet.text)\n",
        "    analysis = TextBlob(tweet.text)\n",
        "    score = SentimentIntensityAnalyzer().polarity_scores(tweet.text)\n",
        "    neg = score['neg']\n",
        "    neu = score['neu']\n",
        "    pos = score['pos']\n",
        "    comp = score['compound']\n",
        "    polarity += analysis.sentiment.polarity\n",
        "    \n",
        "    if neg > pos:\n",
        "      negative_list.append(tweet.text)\n",
        "      negative += 1\n",
        "    elif pos > neg:\n",
        "      positive_list.append(tweet.text)\n",
        "      positive += 1\n",
        "    elif pos == neg:\n",
        "      neutral_list.append(tweet.text)\n",
        "      neutral += 1\n",
        "\n",
        "  positive = percentage(positive, rows_in_group)\n",
        "  negative = percentage(negative, rows_in_group)\n",
        "  neutral = percentage(neutral, rows_in_group)\n",
        "  polarity = percentage(polarity, rows_in_group)\n",
        "  positive = format(positive, '.1f')\n",
        "  negative = format(negative, '.1f')\n",
        "  neutral = format(neutral, '.1f')\n",
        "\n",
        "  #Number of Tweets (Total, Positive, Negative, Neutral)\n",
        "  tweet_list = pd.DataFrame(tweet_list)\n",
        "  neutral_list = pd.DataFrame(neutral_list)\n",
        "  negative_list = pd.DataFrame(negative_list)\n",
        "  positive_list = pd.DataFrame(positive_list)\n",
        "\n",
        "  #Creating PieCart\n",
        "  labels = ['Positive ['+str(positive)+'%]' , 'Neutral ['+str(neutral)+'%]','Negative ['+str(negative)+'%]']\n",
        "  sizes = [positive, neutral, negative]\n",
        "  colors = ['yellowgreen', 'blue','red']\n",
        "  patches, texts = plt.pie(sizes,colors=colors, startangle=90)\n",
        "  plt.style.use('default')\n",
        "  plt.legend(labels)\n",
        "  plt.title(\"Sentiment Analysis Results for short list of Infrastructure Bill Tweets: \" + list(typology_dict.keys())[list(typology_dict.values()).index(group)])\n",
        "  plt.axis('equal')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEf-YFYUHMva"
      },
      "source": [
        "In this dataset, there seems to be a higher percentage of Negative Tweets about the Infrastructure Bill for the Democrats than for the Republicans. You will see why that is when you scan through the Tweets yourself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vCitTct5IAVx"
      },
      "source": [
        "# Some Graph Theory\n",
        "\n",
        "Here, we will use spaCY to parse the Tweets and get an understanding of how similar they are. Much of this was derived [from this Kaggle example.](https://www.kaggle.com/caractacus/thematic-text-analysis-using-spacy-networkx)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "id": "tbeTHtfyIN7T",
        "outputId": "a1da5026-3f22-423e-8ee6-1a0013904d5b"
      },
      "source": [
        "tokens = []\n",
        "lemma = []\n",
        "pos = []\n",
        "parsed_doc = [] \n",
        "col_to_parse = 'text'\n",
        "\n",
        "for doc in nlp.pipe(offline_tweets_df[col_to_parse].astype('unicode').values, batch_size=50,\n",
        "                        n_process=3):\n",
        "    if doc.is_parsed:\n",
        "        parsed_doc.append(doc)\n",
        "        tokens.append([n.text for n in doc])\n",
        "        lemma.append([n.lemma_ for n in doc])\n",
        "        pos.append([n.pos_ for n in doc])\n",
        "    else:\n",
        "        # We want to make sure that the lists of parsed results have the\n",
        "        # same number of entries of the original Dataframe, so add some blanks in case the parse fails\n",
        "        tokens.append(None)\n",
        "        lemma.append(None)\n",
        "        pos.append(None)\n",
        "\n",
        "\n",
        "offline_tweets_df['parsed_doc'] = parsed_doc\n",
        "offline_tweets_df['comment_tokens'] = tokens\n",
        "offline_tweets_df['comment_lemma'] = lemma\n",
        "offline_tweets_df['pos_pos'] = pos\n",
        "offline_tweets_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeagH_-pK5t0"
      },
      "source": [
        "## Remove Stopwords\n",
        "\n",
        "We can reduce increase the signal:noise ratio in these Tweets by removing some of the more common words (or stopwords). By removing these from the tweets, we would prevent them from influencing the analysis of whether two tweets are similar. \n",
        "\n",
        "**For now, let's look at what words are included in spaCy's stopword list.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEfw5UDILEPX",
        "outputId": "30b14df1-b751-4897-9c17-4957099eca15"
      },
      "source": [
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "print('Number of stopwords: %d' % len(stop_words))\n",
        "print(list(stop_words))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOdkwpQ0LSB1"
      },
      "source": [
        "**Now, let's take a look at spaCy's similarity function:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bG82EMVSLdtf",
        "outputId": "7ba713b9-4507-4273-9945-4f2098e542ee"
      },
      "source": [
        "print(offline_tweets_df['parsed_doc'][0].similarity(offline_tweets_df['parsed_doc'][1]))\n",
        "print(offline_tweets_df['parsed_doc'][0].similarity(offline_tweets_df['parsed_doc'][10]))\n",
        "print(offline_tweets_df['parsed_doc'][1].similarity(offline_tweets_df['parsed_doc'][10]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhyL7fc4Mgu0"
      },
      "source": [
        "Now, we can form a graph where each node in the graph represents an individual Tweet. And each edge represents similarity. We start out by making the graph fully connected (all nodes connect to other nodes). And then we remove edges that have similarity below a certain threshold. \n",
        "\n",
        "*Sidenote: I quickly grabbed this example from towardsdatascience.com and ran with it to show you the basics. But there is a much, much better way to do this, performance-wise. Can you figure out what it is?*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQsiIhYmMswC",
        "outputId": "4d06a38b-be46-4ae8-c98b-79e061c27969"
      },
      "source": [
        "# won't scale linearly!                              \n",
        "raw_G = nx.Graph() # undirected\n",
        "n = 0\n",
        "\n",
        "for i in offline_tweets_df['parsed_doc']:        # sure, it's inefficient, but it will do\n",
        "    for j in offline_tweets_df['parsed_doc']:\n",
        "        if i != j:\n",
        "            if not (raw_G.has_edge(j, i)):\n",
        "                sim = i.similarity(j)\n",
        "                raw_G.add_edge(i, j, weight = sim)\n",
        "                n = n + 1\n",
        "\n",
        "print(raw_G.number_of_nodes(), \"nodes, and\", raw_G.number_of_edges(), \"edges created.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EAUsKw8oNQFZ"
      },
      "source": [
        "**Now, let's remove edges with similarity below 0.85**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pOmFVmgNM0H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5ae4950-9cf1-447c-dde9-5e42659eb6a6"
      },
      "source": [
        "edges_to_kill = []\n",
        "min_wt = 0.85      # this is our cutoff value for a minimum edge-weight \n",
        "\n",
        "for n, nbrs in raw_G.adj.items():\n",
        "    #print(\"\\nProcessing origin-node:\", n, \"... \")\n",
        "    for nbr, eattr in nbrs.items():\n",
        "        # remove edges below a certain weight\n",
        "        data = eattr['weight']\n",
        "        if data < min_wt: \n",
        "            # print('(%.3f)' % (data))  \n",
        "            # print('(%d, %d, %.3f)' % (n, nbr, data))  \n",
        "            #print(\"\\nNode: \", n, \"\\n <-\", data, \"-> \", \"\\nNeighbour: \", nbr)\n",
        "            edges_to_kill.append((n, nbr)) \n",
        "            \n",
        "print(\"\\n\", len(edges_to_kill) / 2, \"edges to kill (of\", raw_G.number_of_edges(), \"), before de-duplicating\")\n",
        "for u, v in edges_to_kill:\n",
        "    if raw_G.has_edge(u, v):   # catches (e.g.) those edges where we've removed them using reverse ... (v, u)\n",
        "        raw_G.remove_edge(u, v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIbUw2x-Nw7E"
      },
      "source": [
        "**Now let's visualize this Graph to see how connected it is:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU9YmGPXNwW8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "outputId": "c565eabd-8303-46ad-bc52-4fefc47313f3"
      },
      "source": [
        "strong_G = raw_G\n",
        "nx.draw(strong_G, node_size=20, edge_color='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLABnWH0OVx_"
      },
      "source": [
        "Visualising the whole graph, but only those links of weights above a certain cutoff, allows us to get a feel for a good cutoff level to use when visualising the structure. Having filtered out these lower-weighted links, we can clean up the graph by removing the isolates. This will enable the layout engine to show us more of the structure of the components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_oJWk7gKOl-S"
      },
      "source": [
        "strong_G.remove_nodes_from(list(nx.isolates(strong_G)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8AQLRmPhOfAb"
      },
      "source": [
        "We can also tweak the layout algorithm. By, for example, changing the ideal distance at which the repulsive and attractive forces are in equilibrium. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AE0lRlagOi64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 516
        },
        "outputId": "c3937259-ee5e-46f2-eaab-d4e217f96060"
      },
      "source": [
        "from math import sqrt\n",
        "count = strong_G.number_of_nodes()\n",
        "equilibrium = 10 / sqrt(count)    # default for this is 1/sqrt(n), but this will 'blow out' the layout for better visibility\n",
        "pos = nx.fruchterman_reingold_layout(strong_G, k=equilibrium, iterations=300)\n",
        "nx.draw(strong_G, pos=pos, node_size=10, edge_color='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nx.draw(strong_G, node_size = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}